{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'fcaf67a22c8d', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'esEA-f3TRtS4IPgZCxnPrQ', 'version': {'number': '8.13.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '16cc90cd2d08a3147ce02b07e50894bc060a4cbf', 'build_date': '2024-04-05T14:45:26.420424304Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "\n",
    "password = \"btA+=QjPyyUUx0Tq*T9f\"\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"https\"}],\n",
    "    ca_certs=\"/home/ubuntu/.crts/http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", password),\n",
    ")\n",
    "\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed_index\n"
     ]
    }
   ],
   "source": [
    "indices = es.cat.indices(format='json')\n",
    "\n",
    "# Print the indices\n",
    "for index in indices:\n",
    "    print(index['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "password = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"https\"}],\n",
    "    ca_certs=\"/home/ubuntu/.crts/http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", password),\n",
    ")\n",
    "\n",
    "# Define the index name\n",
    "index_name = \"pubmed_index_embedded\"\n",
    "\n",
    "# Delete the index if it exists\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# Check again if the index exists, and if not, create it\n",
    "if not es.indices.exists(index=index_name):\n",
    "    # Define the mapping\n",
    "    {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_lemmatizer_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"stopwords\", \"lemmatizer_filter\"]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"lemmatizer_filter\": {\n",
    "          \"type\": \"lemmatizer\",  # lemmatization plugin should be installed\n",
    "          \"language\": \"English\"  # Specify the language for lemmatization\n",
    "        },\n",
    "        \"stopwords\": {\n",
    "          \"type\": \"stop\",\n",
    "          \"stopwords\": \"_english_\"  # the built-in English stop words list\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"content\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"custom_lemmatizer_analyzer\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create the index with the defined mapping\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "source_directory = Path('/home/ubuntu/data/pubmed_bioBERT')\n",
    "error_log_path = Path('./errors.jsonl')  # Pfad zur Fehlerprotokolldatei\n",
    "\n",
    "def bulk_index_documents(source_directory, index_name, error_log_path):\n",
    "    if not source_directory.exists():\n",
    "        print(\"The source directory does not exist.\")\n",
    "        return\n",
    "\n",
    "    actions = []  # List to store the documents to be indexed\n",
    "\n",
    "    # Open the error log file for writing\n",
    "    with error_log_path.open('w') as error_log:\n",
    "        # Iterate through each file in the source directory\n",
    "        for file_name in tqdm(list(os.listdir(source_directory))):\n",
    "            if file_name.endswith('.jsonl'):\n",
    "                source_file = source_directory / file_name\n",
    "                \n",
    "                # Open and read the JSONL file\n",
    "                with open(source_file, 'r') as json_file:\n",
    "                    for line in json_file:\n",
    "                        try:\n",
    "                            doc = json.loads(line)\n",
    "                            \n",
    "                            # Remove the \"embeddings\" field from the document\n",
    "                            #if \"embeddings\" in doc:\n",
    "                            #    del doc[\"embeddings\"]\n",
    "                            \n",
    "                            action = {\n",
    "                                \"_index\": index_name,\n",
    "                                \"_source\": doc\n",
    "                            }\n",
    "                            actions.append(action)\n",
    "\n",
    "                            if len(actions) == 200:  # Bulk indexing threshold\n",
    "                                helpers.bulk(es, actions)\n",
    "                                actions = []\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            # Log the error\n",
    "                            error_log.write(f\"Error in file {file_name}: {e}\\n\")\n",
    "                            error_log.write(f\"{line}\\n\")\n",
    "                        except Exception as e:\n",
    "                            error_log.write(f\"Unexpected error in file {file_name}: {e}\\n\")\n",
    "                            error_log.write(f\"{line}\\n\")\n",
    "\n",
    "        # Index any remaining documents\n",
    "        if actions:\n",
    "            helpers.bulk(es, actions)\n",
    "\n",
    "    print('Indexing complete')\n",
    "\n",
    "# Call the function to index the documents\n",
    "bulk_index_documents(source_directory, index_name, error_log_path)\n",
    "\n",
    "# Count and print the number of documents in the index\n",
    "count_result = es.count(index=index_name)\n",
    "print(f\"Index contains {count_result['count']} documents.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
