{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we loop trough each training set for example BioASQ-trainingDataset2b.json and extract the pubmed IDS used to answers questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Files: 100%|██████████| 11/11 [00:09<00:00,  1.21it/s]\n",
      "Aggregating PubMed IDs: 100%|██████████| 43188/43188 [00:00<00:00, 3358304.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "csv_dir = os.path.expanduser(json_dir + '/csv')  # Ensure the path is expanded to the user's home directory\n",
    "\n",
    "# Create the CSV directory if it doesn't exist\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a set to hold all unique PubMed IDs across files\n",
    "all_pubmed_ids = set()\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]  # Ensure the path is expanded\n",
    "\n",
    "# Loop through files with a tqdm progress bar\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON Files\"):\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "\n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize a set for this file's PubMed IDs\n",
    "    file_pubmed_ids = set()\n",
    "\n",
    "    # Extract unique PubMed IDs from the 'questions' section\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        for url in documents:\n",
    "            pubmed_id = int(url.split('/')[-1])\n",
    "            file_pubmed_ids.add(pubmed_id)\n",
    "\n",
    "    # Update the set of all PubMed IDs, since a set can only contain unique numbers the same  PUBMEDIDS wont be stored twice\n",
    "    all_pubmed_ids.update(file_pubmed_ids)\n",
    "\n",
    "    # Save to DataFrame and CSV for this file\n",
    "    df = pd.DataFrame({'pubmedid': list(file_pubmed_ids)})\n",
    "    csv_filename = os.path.splitext(json_file)[0] + '.csv'\n",
    "    csv_path = os.path.join(csv_dir, csv_filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Convert the set to a list with tqdm progress\n",
    "all_pubmed_ids_list = list(tqdm(all_pubmed_ids, desc=\"Aggregating PubMed IDs\"))\n",
    "\n",
    "# Save all PubMed IDs to a DataFrame with an extra column and save to CSV\n",
    "all_pubmed_ids_df = pd.DataFrame({'pubmedid': all_pubmed_ids_list, 'enthalten_in_dataset': 0})\n",
    "complete_csv_path = os.path.join(csv_dir, 'csv_complete.csv')\n",
    "all_pubmed_ids_df.to_csv(complete_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we read in all the pubmedids we currently have loaded into our "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script reads in the before created list of all pubmed ids used to answer questions and all the pubmed ids used in our dataset\n",
    "\n",
    "it then flags all  the pubmedids which are avaible in the questions and our data subset used (remember we created the latter in the previous stepp)\n",
    "\n",
    "in the end we first update the csv_complete.csv on wether or not the pubmedid is containted flag afterwards we save the matched pubmed ids into a seperate file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of PubMed IDs with a 1: 3.035565434843012%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "complete_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/csv_complete.csv' #csv file with all unique pubmed ids that are used to answer questions\n",
    "rag_pubmed_csv_path = '~/data/faiss_indices/bioBERT/PMIDs/concatenated_pubmed_ids.csv' #csv file with all the pubmedids currently in our system\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv' # csv file that results from running this script which containes alll the pubmed ids that \n",
    "# should be able to answer questions \n",
    "\n",
    "# Read the DataFrames\n",
    "complete_df = pd.read_csv(complete_csv_path)\n",
    "# Read the RAGPubMed.csv file assuming it has no header and only one column of integers\n",
    "rag_pubmed_df = pd.read_csv(rag_pubmed_csv_path, header=None, names=['PMID'], dtype={'PMID': int})\n",
    "\n",
    "\n",
    "# Check for presence and update the column\n",
    "complete_df['enthalten_in_dataset'] = complete_df['pubmedid'].isin(rag_pubmed_df['PMID']).astype(int)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "complete_df.to_csv(complete_csv_path, index=False)\n",
    "\n",
    "# Extract the PubMed IDs that have a match (1 in the 'enthalten_in_dataset' column)\n",
    "matched_pubmed_ids = complete_df[complete_df['enthalten_in_dataset'] == 1]['pubmedid']\n",
    "\n",
    "# Save the matched PubMed IDs to a separate CSV file\n",
    "matched_pubmed_ids.to_csv(matched_ids_csv_path, index=False, header=['pubmedid'])\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (complete_df['enthalten_in_dataset'].sum() / len(complete_df)) * 100\n",
    "\n",
    "print(f\"Percentage of PubMed IDs with a 1: {percentage}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract each questions that has at least one pubmed id as answer which is present in our dataset and save it into the json file. we also provide the count of how many questions should be answerable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/Questions_answers_data/total/all_questions_in_system.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m             selected_entries\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save the selected entries to a new JSON file\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_json_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     44\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m: selected_entries}, file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Print the total count of entries and the count of selected entries\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/Questions_answers_data/total/all_questions_in_system.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv'\n",
    "output_json_path = '~/Questions_answers_data/all_questions_in_system.json'\n",
    "\n",
    "# Read the matched PubMed IDs\n",
    "matched_ids_df = pd.read_csv(matched_ids_csv_path)\n",
    "matched_pubmed_ids = set(matched_ids_df['pubmedid'])\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]\n",
    "\n",
    "# Initialize a list to hold entries that meet the criteria and a counter for all entries\n",
    "selected_entries = []\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "    \n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Increment total_entries by the number of questions in the current file\n",
    "    total_entries += len(data.get('questions', []))\n",
    "    \n",
    "    # Check each entry for matched PubMed IDs\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        pubmed_ids = [int(url.split('/')[-1]) for url in documents]\n",
    "        # Count how many PubMed IDs in this question are in the matched list\n",
    "        match_count = sum(id_ in matched_pubmed_ids for id_ in pubmed_ids)\n",
    "        # If at least one (or two) match(es), save the entire entry\n",
    "        if match_count >= 1:  # Change to `>= 2` if you need at least two matches\n",
    "            selected_entries.append(question)\n",
    "\n",
    "# Save the selected entries to a new JSON file\n",
    "with open(os.path.expanduser(output_json_path), 'w') as file:\n",
    "    json.dump({'questions': selected_entries}, file, indent=4)\n",
    "\n",
    "# Print the total count of entries and the count of selected entries\n",
    "print(f\"Total entries in all JSON files: {total_entries}\")\n",
    "print(f\"Total selected entries saved: {len(selected_entries)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
