{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we loop trough each training set for example BioASQ-trainingDataset2b.json and extract the pubmed IDS used to answers questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Files: 100%|██████████| 11/11 [00:09<00:00,  1.21it/s]\n",
      "Aggregating PubMed IDs: 100%|██████████| 43188/43188 [00:00<00:00, 3358304.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "csv_dir = os.path.expanduser(json_dir + '/csv')  # Ensure the path is expanded to the user's home directory\n",
    "\n",
    "# Create the CSV directory if it doesn't exist\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a set to hold all unique PubMed IDs across files\n",
    "all_pubmed_ids = set()\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]  # Ensure the path is expanded\n",
    "\n",
    "# Loop through files with a tqdm progress bar\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON Files\"):\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "\n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize a set for this file's PubMed IDs\n",
    "    file_pubmed_ids = set()\n",
    "\n",
    "    # Extract unique PubMed IDs from the 'questions' section\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        for url in documents:\n",
    "            pubmed_id = int(url.split('/')[-1])\n",
    "            file_pubmed_ids.add(pubmed_id)\n",
    "\n",
    "    # Update the set of all PubMed IDs, since a set can only contain unique numbers the same  PUBMEDIDS wont be stored twice\n",
    "    all_pubmed_ids.update(file_pubmed_ids)\n",
    "\n",
    "    # Save to DataFrame and CSV for this file\n",
    "    df = pd.DataFrame({'pubmedid': list(file_pubmed_ids)})\n",
    "    csv_filename = os.path.splitext(json_file)[0] + '.csv'\n",
    "    csv_path = os.path.join(csv_dir, csv_filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Convert the set to a list with tqdm progress\n",
    "all_pubmed_ids_list = list(tqdm(all_pubmed_ids, desc=\"Aggregating PubMed IDs\"))\n",
    "\n",
    "# Save all PubMed IDs to a DataFrame with an extra column and save to CSV\n",
    "all_pubmed_ids_df = pd.DataFrame({'pubmedid': all_pubmed_ids_list, 'enthalten_in_dataset': 0})\n",
    "complete_csv_path = os.path.join(csv_dir, 'csv_complete.csv')\n",
    "all_pubmed_ids_df.to_csv(complete_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we read in all the pubmedids we currently have loaded into our "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script reads in the before created list of all pubmed ids used to answer questions and all the pubmed ids used in our dataset\n",
    "\n",
    "it then flags all  the pubmedids which are avaible in the questions and our data subset used (remember we created the latter in the previous stepp)\n",
    "\n",
    "in the end we first update the csv_complete.csv on wether or not the pubmedid is containted flag afterwards we save the matched pubmed ids into a seperate file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of PubMed IDs with a 1: 3.035565434843012%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "complete_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/csv_complete.csv' #csv file with all unique pubmed ids that are used to answer questions\n",
    "rag_pubmed_csv_path = '~/data/faiss_indices/bioBERT/PMIDs/concatenated_pubmed_ids.csv' #csv file with all the pubmedids currently in our system\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv' # csv file that results from running this script which containes alll the pubmed ids that \n",
    "# should be able to answer questions \n",
    "\n",
    "# Read the DataFrames\n",
    "complete_df = pd.read_csv(complete_csv_path)\n",
    "# Read the RAGPubMed.csv file assuming it has no header and only one column of integers\n",
    "rag_pubmed_df = pd.read_csv(rag_pubmed_csv_path, header=None, names=['PMID'], dtype={'PMID': int})\n",
    "\n",
    "\n",
    "# Check for presence and update the column\n",
    "complete_df['enthalten_in_dataset'] = complete_df['pubmedid'].isin(rag_pubmed_df['PMID']).astype(int)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "complete_df.to_csv(complete_csv_path, index=False)\n",
    "\n",
    "# Extract the PubMed IDs that have a match (1 in the 'enthalten_in_dataset' column)\n",
    "matched_pubmed_ids = complete_df[complete_df['enthalten_in_dataset'] == 1]['pubmedid']\n",
    "\n",
    "# Save the matched PubMed IDs to a separate CSV file\n",
    "matched_pubmed_ids.to_csv(matched_ids_csv_path, index=False, header=['pubmedid'])\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (complete_df['enthalten_in_dataset'].sum() / len(complete_df)) * 100\n",
    "\n",
    "print(f\"Percentage of PubMed IDs with a 1: {percentage}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract each questions that has at least one pubmed id as answer which is present in our dataset and save it into the json file. we also provide the count of how many questions should be answerable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in all JSON files: 30212\n",
      "Total selected entries saved: 723\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv'\n",
    "output_json_path = '~/Questions_answers_data/all_questions_in_system.json'\n",
    "\n",
    "# Read the matched PubMed IDs\n",
    "matched_ids_df = pd.read_csv(matched_ids_csv_path)\n",
    "matched_pubmed_ids = set(matched_ids_df['pubmedid'])\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]\n",
    "\n",
    "# Initialize a list to hold entries that meet the criteria and a counter for all entries\n",
    "selected_entries = []\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "    \n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Increment total_entries by the number of questions in the current file\n",
    "    total_entries += len(data.get('questions', []))\n",
    "    \n",
    "    # Check each entry for matched PubMed IDs\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        pubmed_ids = [int(url.split('/')[-1]) for url in documents]\n",
    "        # Count how many PubMed IDs in this question are in the matched list\n",
    "        match_count = sum(id_ in matched_pubmed_ids for id_ in pubmed_ids)\n",
    "        # If at least one (or two) match(es), save the entire entry\n",
    "        if match_count >= 1:  # Change to `>= 2` if you need at least two matches ore more\n",
    "            selected_entries.append(question)\n",
    "\n",
    "# Save the selected entries to a new JSON file\n",
    "with open(os.path.expanduser(output_json_path), 'w') as file:\n",
    "    json.dump({'questions': selected_entries}, file, indent=4)\n",
    "\n",
    "# Print the total count of entries and the count of selected entries\n",
    "print(f\"Total entries in all JSON files: {total_entries}\")\n",
    "print(f\"Total selected entries saved: {len(selected_entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note for later:\n",
    "\n",
    "if only one pubmed id required we can answer 4000 questions\n",
    "with at least 2 we can answer 2036\n",
    "with at least 3 1221\n",
    "with at least 4 731\n",
    "with all 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script above does exactly the same as above but only counts the question if all the pubmed ids to answer are in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in all JSON files: 30212\n",
      "Total selected entries saved: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv'\n",
    "output_json_path = '~/Questions_answers_data/all_questions_with_all_ids_matched.json'  # Updated file name\n",
    "\n",
    "# Read the matched PubMed IDs\n",
    "matched_ids_df = pd.read_csv(matched_ids_csv_path)\n",
    "matched_pubmed_ids = set(matched_ids_df['pubmedid'])\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]\n",
    "\n",
    "# Initialize a list to hold entries that meet the criteria and a counter for all entries\n",
    "selected_entries = []\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "    \n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Increment total_entries by the number of questions in the current file\n",
    "    total_entries += len(data.get('questions', []))\n",
    "    \n",
    "    # Check each entry for matched PubMed IDs\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        pubmed_ids = [int(url.split('/')[-1]) for url in documents]\n",
    "        # Check if all PubMed IDs in this question are in the matched list\n",
    "        if all(id_ in matched_pubmed_ids for id_ in pubmed_ids):\n",
    "            selected_entries.append(question)\n",
    "\n",
    "# Save the selected entries to a new JSON file\n",
    "with open(os.path.expanduser(output_json_path), 'w') as file:\n",
    "    json.dump({'questions': selected_entries}, file, indent=4)\n",
    "\n",
    "# Print the total count of entries and the count of selected entries\n",
    "print(f\"Total entries in all JSON files: {total_entries}\")\n",
    "print(f\"Total selected entries saved: {len(selected_entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since i didnt think about it that much before we now proceed to remove the duplicates from all generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 298, Entries after: 298, Duplicates removed: 0\n",
      "Entries per type: {'factoid': 91, 'summary': 57, 'yesno': 87, 'list': 63}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system_min2.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "      f\"Duplicates removed: {result['removed_count']}\")\n",
    "print(\"Entries per type:\", result['type_counts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 4064, Entries after: 593, Duplicates removed: 3471\n",
      "Entries per type: {'yesno': 1145, 'factoid': 1190, 'list': 900, 'summary': 829}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(\n",
    "    f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "    f\"Duplicates removed: {result['removed_count']}\"\n",
    ")\n",
    "print(\"Entries per type:\", result[\"type_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 593, Entries after: 593, Duplicates removed: 0\n",
      "Entries per type: {'yesno': 160, 'factoid': 179, 'list': 131, 'summary': 123}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(\n",
    "    f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "    f\"Duplicates removed: {result['removed_count']}\"\n",
    ")\n",
    "print(\"Entries per type:\", result[\"type_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with empty 'documents': 0\n",
      "Updated JSON saved to /home/ubuntu/questions_answers_data/filtered_min1_questions_in_system.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def read_pubmed_ids_from_csv(csv_path):\n",
    "    pubmed_ids = set()\n",
    "    with open(csv_path, newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            pubmed_ids.add(row[0].strip())  # Assuming the IDs are in the first column\n",
    "    return pubmed_ids\n",
    "\n",
    "\n",
    "def filter_documents_in_json(json_path, pubmed_ids):\n",
    "    with open(json_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    empty_document_count = 0\n",
    "\n",
    "    # Process each question in the JSON data\n",
    "    for question in data[\"questions\"]:\n",
    "        original_documents = question[\"documents\"]\n",
    "        filtered_documents = [\n",
    "            doc for doc in original_documents if doc.split(\"/\")[-1] in pubmed_ids\n",
    "        ]\n",
    "        question[\"documents\"] = filtered_documents\n",
    "\n",
    "        if not filtered_documents:\n",
    "            empty_document_count += 1\n",
    "\n",
    "    return data, empty_document_count\n",
    "\n",
    "\n",
    "def save_updated_json(data, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "    print(f\"Updated JSON saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/home/ubuntu/questions_answers_data/bioASQ_data/trainings_sets/csv/matched_pubmed_ids.csv\"\n",
    "    json_path = \"/home/ubuntu/questions_answers_data/all_questions_in_system.json\"\n",
    "    output_json_path = (\n",
    "        \"/home/ubuntu/questions_answers_data/filtered_min1_questions_in_system.json\"\n",
    "    )\n",
    "\n",
    "    pubmed_ids = read_pubmed_ids_from_csv(csv_path)\n",
    "    filtered_data, empty_count = filter_documents_in_json(json_path, pubmed_ids)\n",
    "\n",
    "    print(f\"Number of questions with empty 'documents': {empty_count}\")\n",
    "    save_updated_json(filtered_data, output_json_path)  # Save the updated JSON\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with empty 'documents': 0\n",
      "Updated JSON saved to /home/ubuntu/questions_answers_data/filtered_min2_questions_in_system.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def read_pubmed_ids_from_csv(csv_path):\n",
    "    pubmed_ids = set()\n",
    "    with open(csv_path, newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            pubmed_ids.add(row[0].strip())  # Assuming the IDs are in the first column\n",
    "    return pubmed_ids\n",
    "\n",
    "\n",
    "def filter_documents_in_json(json_path, pubmed_ids):\n",
    "    with open(json_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    empty_document_count = 0\n",
    "\n",
    "    # Process each question in the JSON data\n",
    "    for question in data[\"questions\"]:\n",
    "        original_documents = question[\"documents\"]\n",
    "        filtered_documents = [\n",
    "            doc for doc in original_documents if doc.split(\"/\")[-1] in pubmed_ids\n",
    "        ]\n",
    "        question[\"documents\"] = filtered_documents\n",
    "\n",
    "        if not filtered_documents:\n",
    "            empty_document_count += 1\n",
    "\n",
    "    return data, empty_document_count\n",
    "\n",
    "\n",
    "def save_updated_json(data, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "    print(f\"Updated JSON saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/home/ubuntu/questions_answers_data/bioASQ_data/trainings_sets/csv/matched_pubmed_ids.csv\"\n",
    "    json_path = \"/home/ubuntu/questions_answers_data/all_questions_in_system_min2.json\"\n",
    "    output_json_path = (\n",
    "        \"/home/ubuntu/questions_answers_data/filtered_min2_questions_in_system.json\"\n",
    "    )\n",
    "\n",
    "    pubmed_ids = read_pubmed_ids_from_csv(csv_path)\n",
    "    filtered_data, empty_count = filter_documents_in_json(json_path, pubmed_ids)\n",
    "\n",
    "    print(f\"Number of questions with empty 'documents': {empty_count}\")\n",
    "    save_updated_json(filtered_data, output_json_path)  # Save the updated JSON\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a script that extract 20 list type questions in order to find a optimal solution how to evalute those kinda questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'listexample.json' has been created with 20 'list' type entries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(\n",
    "    \"/home/ubuntu/questions_answers_data/filtered_min2_questions_in_system.json\", \"r\"\n",
    ") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter entries where the type is \"list\"\n",
    "list_entries = [entry for entry in data[\"questions\"] if entry[\"type\"] == \"list\"]\n",
    "\n",
    "# If there are more than 20 list entries, limit to the first 20\n",
    "if len(list_entries) > 20:\n",
    "    list_entries = list_entries[:20]\n",
    "\n",
    "# Create a new dictionary with these entries\n",
    "output_data = {\"questions\": list_entries}\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open(\"/home/ubuntu/questions_answers_data/listexample.json\", \"w\") as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)\n",
    "\n",
    "print(\"File 'listexample.json' has been created with 20 'list' type entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'listexample.json' has been created with 20 'factoid' type entries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(\n",
    "    \"/home/ubuntu/questions_answers_data/filtered_min2_questions_in_system.json\", \"r\"\n",
    ") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter entries where the type is \"list\"\n",
    "list_entries = [entry for entry in data[\"questions\"] if entry[\"type\"] == \"factoid\"]\n",
    "\n",
    "# If there are more than 20 list entries, limit to the first 20\n",
    "if len(list_entries) > 20:\n",
    "    list_entries = list_entries[:20]\n",
    "\n",
    "# Create a new dictionary with these entries\n",
    "output_data = {\"questions\": list_entries}\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open(\"/home/ubuntu/questions_answers_data/factoidexample.json\", \"w\") as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)\n",
    "\n",
    "print(\"File 'listexample.json' has been created with 20 'factoid' type entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 87 entries to /home/ubuntu/questions_answers_data/Experimental_data_min_2/yesno_questions.json\n",
      "Written 63 entries to /home/ubuntu/questions_answers_data/Experimental_data_min_2/list_questions.json\n",
      "Written 57 entries to /home/ubuntu/questions_answers_data/Experimental_data_min_2/summary_questions.json\n",
      "Written 91 entries to /home/ubuntu/questions_answers_data/Experimental_data_min_2/factoid_questions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def process_json(input_file, output_dir):\n",
    "    with open(input_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize dictionaries to hold each type of question\n",
    "    questions_by_type = {\"yesno\": [], \"list\": [], \"summary\": [], \"factoid\": []}\n",
    "\n",
    "    # Sort questions into their respective type lists\n",
    "    for question in data[\"questions\"]:\n",
    "        question_type = question[\"type\"]\n",
    "        if question_type in questions_by_type:\n",
    "            questions_by_type[question_type].append(question)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Unrecognized question type '{question_type}' found and will be ignored.\"\n",
    "            )\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Write each group to a separate file\n",
    "    for type_key, questions in questions_by_type.items():\n",
    "        output_file_name = os.path.join(output_dir, f\"{type_key}_questions.json\")\n",
    "        with open(output_file_name, \"w\") as file:\n",
    "            json.dump(questions, file, indent=4)\n",
    "        print(f\"Written {len(questions)} entries to {output_file_name}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "process_json(\n",
    "    \"/home/ubuntu/questions_answers_data/filtered_min2_questions_in_system.json\",\n",
    "    \"/home/ubuntu/questions_answers_data/Experimental_data_min_2\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
