{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we import some neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from RAG_evaluator import RAG_evaluator\n",
    "sys.path.append(\"../../rag_system/\")\n",
    "from med_rag import MedRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we define an experiment name, this name should ! uniqely! identify the experiemnal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiment_debugginglist_questions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we implement a running experiment by using rag system one two and three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'experiment_bioASQ_min3' already exists at /home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min3\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the new folder will be created\n",
    "base_directory = \"/home/ubuntu/questions_answers_data/experiment_results\"\n",
    "# input directory, change if diffrent one is used\n",
    "question_input_dir = \"/home/ubuntu/questions_answers_data/all_questions_in_system_min3.json\"\n",
    "\n",
    "\n",
    "# Construct the path for the new experiment folder\n",
    "experiment_folder_path = os.path.join(base_directory, experiment_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    os.makedirs(experiment_folder_path)\n",
    "    print(f\"Directory '{experiment_name}' created at {experiment_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{experiment_name}' already exists at {experiment_folder_path}\")\n",
    "\n",
    "# Construct the path for the JSON file\n",
    "output_path_retriever_1 = os.path.join(experiment_folder_path, \"result_ragver_1.json\")\n",
    "output_path_retriever_2 = os.path.join(experiment_folder_path, \"result_ragver_2.json\")\n",
    "output_path_retriever_3 = os.path.join(experiment_folder_path, \"result_ragver_3.json\")\n",
    "output_path_retriever_4 = os.path.join(experiment_folder_path, \"result_ragver_4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the 3 retriever types used in the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retriever 1: BioBERT\n",
    "- Retriever 2: BM25\n",
    "- Retriever 3: Hybrid Retriever BM25 reranked with medCPT cross encoder\n",
    "- Retriever 4: medCPT Retriever with reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever 2: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=2, question_type=2)\n",
    "\n",
    "rag_type = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_dir,\n",
    "    output_path=output_path_retriever_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 2\n",
      "Total Questions: 372\n",
      "\n",
      "Response Time:\n",
      "Mean: 2.55 seconds\n",
      "Standard Deviation: 0.95 seconds\n",
      "\n",
      "Summary of non answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - RAG:\n",
      "Accuracy: 0.85\n",
      "Recall: 0.85\n",
      "Precision: 0.89\n",
      "F1 Score: 0.87\n",
      "\n",
      "Metrics - Retriever:\n",
      "Recall Retriever: 2.20\n",
      "Precision Retriever: 0.30\n",
      "F1 Score Retriever: 0.53\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time overall: 2.55\n",
      "Mean response time retriever: 0.06\n",
      "Mean response time generation: 2.49\n"
     ]
    }
   ],
   "source": [
    "rag_type.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min3/result_ragver_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall des Retrievers: 0.20\n",
      "Precision des Retrievers: 0.60\n",
      "F1-Score des Retrievers: 0.30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Daten in einem DataFrame\n",
    "data = {\n",
    "    \"pmids_ground_truth\": [\n",
    "        \"23083810\", \"22718819\", \"21969101\", \"18809619\", \n",
    "        \"17961216\", \"17881586\", \"17187982\", \"12711673\", \n",
    "        \"10102819\", \"9223347\", \"7479046\", \"2010914\", \n",
    "        \"2207158\", \"2103444\", \"2691247\"\n",
    "    ],\n",
    "    \"matching_retrieved_ids\": ['2010914', '2103444', '2691247'],\n",
    "    \"pmids_retrieved\": ['2010914', '2103444', '2691247', 'some_other_id', 'another_id']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Zugriff auf die Listen innerhalb des DataFrames\n",
    "ground_truth_pmids = list(df[\"pmids_ground_truth\"][0])\n",
    "matching_retrieved_ids = list(df[\"matching_retrieved_ids\"][0])\n",
    "retrieved_pmids = list(df[\"pmids_retrieved\"][0])\n",
    "\n",
    "# Berechnung des Recalls\n",
    "recall_retriever = len(matching_retrieved_ids) / len(ground_truth_pmids)\n",
    "print(f\"Recall des Retrievers: {recall_retriever:.2f}\")\n",
    "\n",
    "# Berechnung der Precision\n",
    "precision_retriever = len(matching_retrieved_ids) / len(retrieved_pmids)\n",
    "print(f\"Precision des Retrievers: {precision_retriever:.2f}\")\n",
    "\n",
    "# Berechnung des F1-Scores\n",
    "if precision_retriever + recall_retriever != 0:\n",
    "    f1_retriever = 2 * (precision_retriever * recall_retriever) / (precision_retriever + recall_retriever)\n",
    "else:\n",
    "    f1_retriever = 0.0\n",
    "\n",
    "print(f\"F1-Score des Retrievers: {f1_retriever:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i use this space to debugg the list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=3, question_type=2)\n",
    "\n",
    "rag_type = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=\"/home/ubuntu/questions_answers_data/listexample.json\",\n",
    "    output_path=\"/home/ubuntu/questions_answers_data/listexample_result_trial2.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  45%|████▌     | 9/20 [16:26<04:29, 24.46s/it]   "
     ]
    }
   ],
   "source": [
    "rag_type.run_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
