{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we import some neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from RAG_evaluator import RAG_evaluator\n",
    "sys.path.append(\"../../rag_system/\")\n",
    "from med_rag import MedRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we define an experiment name, this name should ! uniqely! identify the experiemnal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiment_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we implement a running experiment by using rag system one two and three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'experiment_3' already exists at /home/ubuntu/questions_answers_data/experiment_results/experiment_3\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the new folder will be created\n",
    "base_directory = \"/home/ubuntu/questions_answers_data/experiment_results\"\n",
    "# input directory, change if diffrent one is used\n",
    "question_input_dir =   \"/home/ubuntu/questions_answers_data/all_questions_in_system_min4.json\"\n",
    "\n",
    "\n",
    "# Construct the path for the new experiment folder\n",
    "experiment_folder_path = os.path.join(base_directory, experiment_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    os.makedirs(experiment_folder_path)\n",
    "    print(f\"Directory '{experiment_name}' created at {experiment_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{experiment_name}' already exists at {experiment_folder_path}\")\n",
    "\n",
    "# Construct the path for the JSON file\n",
    "output_path_retriever_1 = os.path.join(experiment_folder_path, \"result_ragver_1.json\")\n",
    "output_path_retriever_2 = os.path.join(experiment_folder_path, \"result_ragver_2.json\")\n",
    "output_path_retriever_3 = os.path.join(experiment_folder_path, \"result_ragver_3.json\")\n",
    "output_path_retriever_4 = os.path.join(experiment_folder_path, \"result_ragver_4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the 3 retriever types used in the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through all questions which are possible to answer with our 10% corpus, and saving the results as a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_1 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_1,\n",
    ")\n",
    "rag_type_1.run_eval(retriever_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 1 correct PMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 1\n",
      "Total Questions: 1098\n",
      "\n",
      "Response Time:\n",
      "Mean: 1.78 seconds\n",
      "Standard Deviation: 0.42 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.54\n",
      "Recall: 0.27\n",
      "Precision: 0.24\n",
      "F1 Score: 0.21\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.07\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_1 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_1,\n",
    ")\n",
    "rag_type_1.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_1/result_ragver_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 4 correct PMIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 1\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 1.98 seconds\n",
      "Standard Deviation: 0.51 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.49\n",
      "Recall: 0.35\n",
      "Precision: 0.32\n",
      "F1 Score: 0.25\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.09\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_1.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rag Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through all questions which are possible to answer with our 10% corpus, and saving the results as a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_2 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_2.run_eval(retriever_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 1 correct PMID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 2\n",
      "Total Questions: 989\n",
      "\n",
      "Response Time:\n",
      "Mean: 2.46 seconds\n",
      "Standard Deviation: 0.85 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.70\n",
      "Recall: 0.30\n",
      "Precision: 0.26\n",
      "F1 Score: 0.26\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.28\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_2.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_1/result_ragver_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 4 correct PMIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 2\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 2.47 seconds\n",
      "Standard Deviation: 0.89 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.68\n",
      "Recall: 0.32\n",
      "Precision: 0.35\n",
      "F1 Score: 0.33\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 1.00\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_2.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Type 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_3 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_3.run_eval(retriever_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 3\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 6.00 seconds\n",
      "Standard Deviation: 10.04 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.51\n",
      "Recall: 0.35\n",
      "Precision: 0.33\n",
      "F1 Score: 0.27\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.11\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_3.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=4, question_type=2)\n",
    "\n",
    "rag_type_4 = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_dir,\n",
    "    output_path=output_path_retriever_4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_4.run_eval(retriever_type=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 4\n",
      "Total Questions: 238\n",
      "\n",
      "Response Time:\n",
      "Mean: 4.31 seconds\n",
      "Standard Deviation: 1.05 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.76\n",
      "Recall: 0.43\n",
      "Precision: 0.38\n",
      "F1 Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "rag_type_4.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_3/result_ragver_4.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
