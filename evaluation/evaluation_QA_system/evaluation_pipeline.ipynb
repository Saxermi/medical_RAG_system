{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we import some neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from RAG_evaluator import RAG_evaluator\n",
    "sys.path.append(\"../../rag_system/\")\n",
    "from med_rag import MedRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we define an experiment name, this name should ! uniqely! identify the experiemnal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiment_debugginglist_questions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we implement a running experiment by using rag system one two and three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'experiment_debugginglist_questions' created at /home/ubuntu/questions_answers_data/experiment_results/experiment_debugginglist_questions\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the new folder will be created\n",
    "base_directory = \"/home/ubuntu/questions_answers_data/experiment_results\"\n",
    "# input directory, change if diffrent one is used\n",
    "question_input_dir = \"/home/ubuntu/questions_answers_data/all_questions_in_system_min3.json\"\n",
    "\n",
    "\n",
    "# Construct the path for the new experiment folder\n",
    "experiment_folder_path = os.path.join(base_directory, experiment_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    os.makedirs(experiment_folder_path)\n",
    "    print(f\"Directory '{experiment_name}' created at {experiment_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{experiment_name}' already exists at {experiment_folder_path}\")\n",
    "\n",
    "# Construct the path for the JSON file\n",
    "output_path_retriever_1 = os.path.join(experiment_folder_path, \"result_ragver_1.json\")\n",
    "output_path_retriever_2 = os.path.join(experiment_folder_path, \"result_ragver_2.json\")\n",
    "output_path_retriever_3 = os.path.join(experiment_folder_path, \"result_ragver_3.json\")\n",
    "output_path_retriever_4 = os.path.join(experiment_folder_path, \"result_ragver_4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the 3 retriever types used in the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retriever 1: BioBERT\n",
    "- Retriever 2: BM25\n",
    "- Retriever 3: Hybrid Retriever BM25 reranked with medCPT cross encoder\n",
    "- Retriever 4: medCPT Retriever with reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever 2: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=2, question_type=2)\n",
    "\n",
    "rag_type = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_dir,\n",
    "    output_path=output_path_retriever_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 4\n",
      "Total Questions: 85\n",
      "\n",
      "Response Time:\n",
      "Mean: 3.81 seconds\n",
      "Standard Deviation: 1.42 seconds\n",
      "\n",
      "Summary of non-answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - RAG Q&A:\n",
      "Accuracy: 0.86\n",
      "Recall: 0.86\n",
      "Precision: 0.87\n",
      "F1 Score: 0.86\n",
      "\n",
      "Metrics - Retriever:\n",
      "Recall Retriever: 0.32\n",
      "Precision Retriever: 0.01\n",
      "F1 Score Retriever: 0.02\n",
      "\n",
      "Metrics - Used vs Retrieved:\n",
      "Recall Used vs Retrieved: 0.49\n",
      "Precision Used vs Retrieved: 0.01\n",
      "F1 Score Used vs Retrieved: 0.02\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time retriever: 2.33\n",
      "Standard deviation response time retriever: 0.71\n",
      "Mean response time generation: 1.49\n",
      "Standard deviation response time generation: 1.33\n"
     ]
    }
   ],
   "source": [
    "rag_type.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min2/result_ragver_4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of all question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'experiment_bioASQ_min1_bioBERT' already exists at /home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min1_bioBERT\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"experiment_bioASQ_min1_bioBERT\"\n",
    "# Base directory where the new folder will be created\n",
    "base_directory = \"/home/ubuntu/questions_answers_data/experiment_results\"\n",
    "# input directory, change if diffrent one is used\n",
    "question_input_factoid = \"/home/ubuntu/questions_answers_data/bioASQ_data_min_1/factoid_questions.json\"\n",
    "question_input_summary = \"/home/ubuntu/questions_answers_data/bioASQ_data_min_1/summary_questions.json\"\n",
    "question_input_list = \"/home/ubuntu/questions_answers_data/bioASQ_data_min_1/list_questions.json\"\n",
    "question_input_yesno = \"/home/ubuntu/questions_answers_data/bioASQ_data_min_1/yesno_questions.json\"\n",
    "\n",
    "# Construct the path for the new experiment folder\n",
    "experiment_folder_path = os.path.join(base_directory, experiment_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    os.makedirs(experiment_folder_path)\n",
    "    print(f\"Directory '{experiment_name}' created at {experiment_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{experiment_name}' already exists at {experiment_folder_path}\")\n",
    "\n",
    "# Construct the path for the JSON file\n",
    "output_path_question_factoid = os.path.join(experiment_folder_path, \"result_factoid.json\")\n",
    "output_path_question_summary = os.path.join(experiment_folder_path, \"result_summary.json\")\n",
    "output_path_question_list = os.path.join(experiment_folder_path, \"result_list.json\")\n",
    "output_path_question_yesno = os.path.join(experiment_folder_path, \"result_yesno.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Factoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=1, question_type=1)\n",
    "\n",
    "eval_factoid = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_factoid,\n",
    "    output_path=output_path_question_factoid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_factoid.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever Unknown\n",
      "Total Questions: 175\n",
      "\n",
      "Response Time:\n",
      "Mean: 6.39 seconds\n",
      "Standard Deviation: 1.66 seconds\n",
      "\n",
      "Summary of non-answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - Retriever:\n",
      "Average Recall: 0.58\n",
      "\n",
      "Metrics for RAG Usage:\n",
      "Average Precision: 0.34\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time retriever: 4.16\n",
      "Standard deviation response time retriever: 1.17\n",
      "Mean response time generation: 2.22\n",
      "Standard deviation response time generation: 1.05\n"
     ]
    }
   ],
   "source": [
    "eval_factoid.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min1_hybrid/result_factoid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=1, question_type=1)\n",
    "\n",
    "eval_summary = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_summary,\n",
    "    output_path=output_path_question_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever Unknown\n",
      "Total Questions: 121\n",
      "\n",
      "Response Time:\n",
      "Mean: 7.50 seconds\n",
      "Standard Deviation: 1.88 seconds\n",
      "\n",
      "Summary of non-answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - Retriever:\n",
      "Average Recall: 0.59\n",
      "\n",
      "Metrics for RAG Usage:\n",
      "Average Precision: 0.32\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time retriever: 4.22\n",
      "Standard deviation response time retriever: 1.33\n",
      "Mean response time generation: 3.28\n",
      "Standard deviation response time generation: 1.25\n"
     ]
    }
   ],
   "source": [
    "eval_summary.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min1_hybrid/result_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=1, question_type=4)\n",
    "\n",
    "eval_list = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_list,\n",
    "    output_path=output_path_question_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever Unknown\n",
      "Total Questions: 131\n",
      "\n",
      "Response Time:\n",
      "Mean: 5.94 seconds\n",
      "Standard Deviation: 1.45 seconds\n",
      "\n",
      "Summary of non-answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - Retriever:\n",
      "Average Recall: 0.51\n",
      "\n",
      "Metrics for RAG Usage:\n",
      "Average Precision: 0.34\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time retriever: 4.37\n",
      "Standard deviation response time retriever: 1.29\n",
      "Mean response time generation: 1.57\n",
      "Standard deviation response time generation: 0.7\n"
     ]
    }
   ],
   "source": [
    "eval_list.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min1_hybrid/result_list.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Yes/No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=1, question_type=2)\n",
    "\n",
    "eval_yesno = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_yesno,\n",
    "    output_path=output_path_question_yesno,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_yesno.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever Unknown\n",
      "Total Questions: 160\n",
      "\n",
      "Response Time:\n",
      "Mean: 5.80 seconds\n",
      "Standard Deviation: 1.30 seconds\n",
      "\n",
      "Summary of non-answered questions:\n",
      "Absolute count - No Docs Found: 0\n",
      "Percentage - No Docs Found: 0.00%\n",
      "\n",
      "Metrics - RAG Q&A:\n",
      "Accuracy: 0.86\n",
      "Recall: 0.86\n",
      "Precision: 0.89\n",
      "F1 Score: 0.86\n",
      "\n",
      "Metrics - Retriever:\n",
      "Average Recall: 0.54\n",
      "\n",
      "Metrics for RAG Usage:\n",
      "Average Precision: 0.28\n",
      "\n",
      "Additional metrics:\n",
      "Mean response time retriever: 4.66\n",
      "Standard deviation response time retriever: 1.24\n",
      "Mean response time generation: 1.14\n",
      "Standard deviation response time generation: 0.38\n"
     ]
    }
   ],
   "source": [
    "eval_yesno.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_bioASQ_min1_hybrid/result_yesno.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
