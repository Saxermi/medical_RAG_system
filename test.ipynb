{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:04:34.100226Z",
     "start_time": "2024-03-11T19:04:34.096359Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import json\n",
    "import codecs\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying to extract sample out of 28 GB JSON file by parsing with ijson.\n",
    "\n",
    "JSON structure:\n",
    "\n",
    "{\"articles\": [\n",
    "\t{\"abstractText\":\"text..\", \"journal\":\"journal..\", \"meshMajor\":[\"mesh1\",...,\"meshN\"], \"pmid\":\"PMID\", \"title\":\"title..\", \"year\":\"YYYY\"},\n",
    "\t..., \n",
    "\t{..}\n",
    "]}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3325ce7361c256b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wegen ***IncompleteJSONError*** muss File umcodiert werden. Die Funktion liest die Datei in Chunks und versucht, jeden Chunk als UTF-8 zu dekodieren. Ungültige Byte-Sequenzen werden durch das Unicode-Ersatzzeichen ersetzt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a2150f6c35893e9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fortschritt: 100.00%\n",
      "Bereinigung abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/allMeSH_2022/allMeSH_2022.json\"\n",
    "\n",
    "def clean_json_file(input_filename, output_filename):\n",
    "    total_size = os.path.getsize(input_filename)\n",
    "    processed_size = 0\n",
    "\n",
    "    with open(input_filename, 'rb') as input_file, open(output_filename, 'wb') as output_file:\n",
    "        while True:\n",
    "            chunk = input_file.read(1024 * 1024)  # Lese die Datei in 1MB Chunks\n",
    "            if not chunk:\n",
    "                break  # Beende, wenn das Ende der Datei erreicht ist\n",
    "\n",
    "            clean_chunk = chunk.decode('utf-8', errors='replace').encode('utf-8')\n",
    "            output_file.write(clean_chunk)\n",
    "\n",
    "            processed_size += len(chunk)\n",
    "            progress = (processed_size / total_size) * 100\n",
    "            print(f\"\\rFortschritt: {progress:.2f}%\", end='')\n",
    "\n",
    "    print(\"\\nBereinigung abgeschlossen.\")\n",
    "\n",
    "clean_json_file(filename, 'bereinigte_datei.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:59:08.385320Z",
     "start_time": "2024-03-11T18:58:05.781552Z"
    }
   },
   "id": "5d2b6d197ddb7924",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extrahieren eines Sample-Chunks aus der 28 GB JSON Datei"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4031e6e1799e946"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Artikel wurden in '10k_sample_article.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/bereinigte_datei.json\"\n",
    "articles = []\n",
    "max_articles = 1000\n",
    "\n",
    "with open(filename, 'rb') as input_file:\n",
    "    # Hier passen wir den Pfad an, um die Artikel direkt zu erreichen\n",
    "    parser = ijson.items(input_file, 'articles.item')\n",
    "    for article in parser:\n",
    "        articles.append(article)\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "\n",
    "# speichern des samples\n",
    "\n",
    "# Speichere die extrahierten Artikel in einer neuen JSON-Datei\n",
    "with open('data/10k_sample_article.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(articles, output_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"{len(articles)} Artikel wurden in '10k_sample_article.json' gespeichert.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:16:16.956329Z",
     "start_time": "2024-03-11T19:16:16.866546Z"
    }
   },
   "id": "23773d2f0967949f",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MeSH\n",
    "\n",
    "Durch MeSH sollen Beziehungen bzw. Kanten im Graphen Modelliert werden.\n",
    "\n",
    "- MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed.\n",
    "\n",
    "Überprüfung ob alle Artikel ein Attribut \"meshMajor\" mit mindestens 1 Eintrag haben."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68bab50d9b34c29"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Artikel haben das 'meshMajor'-Attribut mit mindestens einem Eintrag.\n"
     ]
    }
   ],
   "source": [
    "# Datei laden, die die extrahierten Artikel enthält\n",
    "filename = 'data/10k_sample_article.json'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Überprüfe jedes Artikelobjekt auf das Vorhandensein und den Inhalt des \"meshMajor\"-Attributs\n",
    "missing_or_empty_meshMajor = [article for article in articles if not article.get('meshMajor')]\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "if missing_or_empty_meshMajor:\n",
    "    print(f\"{len(missing_or_empty_meshMajor)} von {len(articles)} Artikeln haben kein 'meshMajor'-Attribut oder es ist leer.\")\n",
    "else:\n",
    "    print(\"Alle Artikel haben das 'meshMajor'-Attribut mit mindestens einem Eintrag.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:16:22.668566Z",
     "start_time": "2024-03-11T19:16:22.637640Z"
    }
   },
   "id": "a9c3d68798b5a6d0",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First Import in Neo4j"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d324fc8bd997fbc0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mneo4j\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GraphDatabase\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'neo4j'"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:18:22.369554Z",
     "start_time": "2024-03-11T19:18:22.347721Z"
    }
   },
   "id": "524c6fcfa9f8fe9f",
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
