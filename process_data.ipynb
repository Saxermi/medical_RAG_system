{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:04:34.100226Z",
     "start_time": "2024-03-11T19:04:34.096359Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import json\n",
    "import codecs\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying to extract sample out of 28 GB JSON file by parsing with ijson.\n",
    "\n",
    "JSON structure:\n",
    "\n",
    "{\"articles\": [\n",
    "\t{\"abstractText\":\"text..\", \"journal\":\"journal..\", \"meshMajor\":[\"mesh1\",...,\"meshN\"], \"pmid\":\"PMID\", \"title\":\"title..\", \"year\":\"YYYY\"},\n",
    "\t..., \n",
    "\t{..}\n",
    "]}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3325ce7361c256b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wegen ***IncompleteJSONError*** muss File umcodiert werden. Die Funktion liest die Datei in Chunks und versucht, jeden Chunk als UTF-8 zu dekodieren. Ungültige Byte-Sequenzen werden durch das Unicode-Ersatzzeichen ersetzt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a2150f6c35893e9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fortschritt: 100.00%\n",
      "Bereinigung abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/allMeSH_2022/allMeSH_2022.json\"\n",
    "\n",
    "def clean_json_file(input_filename, output_filename):\n",
    "    total_size = os.path.getsize(input_filename)\n",
    "    processed_size = 0\n",
    "\n",
    "    with open(input_filename, 'rb') as input_file, open(output_filename, 'wb') as output_file:\n",
    "        while True:\n",
    "            chunk = input_file.read(1024 * 1024)  # Lese die Datei in 1MB Chunks\n",
    "            if not chunk:\n",
    "                break  # Beende, wenn das Ende der Datei erreicht ist\n",
    "\n",
    "            clean_chunk = chunk.decode('utf-8', errors='replace').encode('utf-8')\n",
    "            output_file.write(clean_chunk)\n",
    "\n",
    "            processed_size += len(chunk)\n",
    "            progress = (processed_size / total_size) * 100\n",
    "            print(f\"\\rFortschritt: {progress:.2f}%\", end='')\n",
    "\n",
    "    print(\"\\nBereinigung abgeschlossen.\")\n",
    "\n",
    "clean_json_file(filename, 'bereinigte_datei.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:59:08.385320Z",
     "start_time": "2024-03-11T18:58:05.781552Z"
    }
   },
   "id": "5d2b6d197ddb7924",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extrahieren eines Sample-Chunks aus der 28 GB JSON Datei"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4031e6e1799e946"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Artikel wurden in '10k_sample_article.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/bereinigte_datei.json\"\n",
    "articles = []\n",
    "max_articles = 1000\n",
    "\n",
    "with open(filename, 'rb') as input_file:\n",
    "    # Hier passen wir den Pfad an, um die Artikel direkt zu erreichen\n",
    "    parser = ijson.items(input_file, 'articles.item')\n",
    "    for article in parser:\n",
    "        articles.append(article)\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "\n",
    "# speichern des samples\n",
    "\n",
    "# Speichere die extrahierten Artikel in einer neuen JSON-Datei\n",
    "with open('data/10k_sample_article.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(articles, output_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"{len(articles)} Artikel wurden in '10k_sample_article.json' gespeichert.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:21:39.652358Z",
     "start_time": "2024-03-11T19:21:39.593974Z"
    }
   },
   "id": "23773d2f0967949f",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'journal': 'Biosensors',\n 'meshMajor': ['Cell Separation',\n  'Lab-On-A-Chip Devices',\n  'Leukocytes',\n  'Microfluidic Analytical Techniques',\n  'Microfluidics'],\n 'year': '2021',\n 'abstractText': 'Rapid isolation of white blood cells (WBCs) from whole blood is an essential part of any WBC examination platform. However, most conventional cell separation techniques are labor-intensive and low throughput, require large volumes of samples, need extensive cell manipulation, and have low purity. To address these challenges, we report the design and fabrication of a passive, label-free microfluidic device with a unique U-shaped cross-section to separate WBCs from whole blood using hydrodynamic forces that exist in a microchannel with curvilinear geometry. It is shown that the spiral microchannel with a U-shaped cross-section concentrates larger blood cells (e.g., WBCs) in the inner cross-section of the microchannel by moving smaller blood cells (e.g., RBCs and platelets) to the outer microchannel section and preventing them from returning to the inner microchannel section. Therefore, it overcomes the major limitation of a rectangular cross-section where secondary Dean vortices constantly enforce particles throughout the entire cross-section and decrease its isolation efficiency. Under optimal settings, we managed to isolate more than 95% of WBCs from whole blood under high-throughput (6 mL/min), high-purity (88%), and high-capacity (360 mL of sample in 1 h) conditions. High efficiency, fast processing time, and non-invasive WBC isolation from large blood samples without centrifugation, RBC lysis, cell biomarkers, and chemical pre-treatments make this method an ideal choice for downstream cell study platforms.',\n 'pmid': '34821622',\n 'title': 'High-Throughput, Label-Free Isolation of White Blood Cells from Whole Blood Using Parallel Spiral Microchannels with U-Shaped Cross-Section.'}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:46:19.713082Z",
     "start_time": "2024-03-11T19:46:19.705940Z"
    }
   },
   "id": "ffc8ccba9aa9d43c",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MeSH\n",
    "\n",
    "Durch MeSH sollen Beziehungen bzw. Kanten im Graphen Modelliert werden.\n",
    "\n",
    "- MeSH (Medical Subject Headings) ist der NLM-kontrollierte Vokabelthesaurus, der zur Indexierung von Artikeln für PubMed verwendet wird.\n",
    "\n",
    "- Zusätzlich wären Cites von Papers als Kanten interessant. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68bab50d9b34c29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Überprüfung ob alle Artikel ein Attribut \"meshMajor\" mit mindestens 1 Eintrag haben."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3cdc593ff9d948"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Artikel haben das 'meshMajor'-Attribut mit mindestens einem Eintrag.\n"
     ]
    }
   ],
   "source": [
    "# Datei laden, die die extrahierten Artikel enthält\n",
    "filename = 'data/10k_sample_article.json'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Überprüfe jedes Artikelobjekt auf das Vorhandensein und den Inhalt des \"meshMajor\"-Attributs\n",
    "missing_or_empty_meshMajor = [article for article in articles if not article.get('meshMajor')]\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "if missing_or_empty_meshMajor:\n",
    "    print(f\"{len(missing_or_empty_meshMajor)} von {len(articles)} Artikeln haben kein 'meshMajor'-Attribut oder es ist leer.\")\n",
    "else:\n",
    "    print(\"Alle Artikel haben das 'meshMajor'-Attribut mit mindestens einem Eintrag.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:16:22.668566Z",
     "start_time": "2024-03-11T19:16:22.637640Z"
    }
   },
   "id": "a9c3d68798b5a6d0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Einträge fehlen im Attribut 'year'.\n",
      "Einige Artikel haben fehlende oder leere Attribute.\n"
     ]
    }
   ],
   "source": [
    "# Definiere die erwarteten Attribute jedes Artikels\n",
    "erwartete_attribute = [\"pmid\", \"title\", \"abstractText\", \"year\", \"journal\", \"meshMajor\"]\n",
    "\n",
    "# Lade die JSON-Datei\n",
    "filename = 'data/10k_sample_article.json'\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Initialisiere ein Dictionary, um die fehlenden Einträge zu zählen\n",
    "fehlende_eintrage = {attribut: 0 for attribut in erwartete_attribute}\n",
    "\n",
    "# Überprüfe jedes Attribut in jedem Artikel\n",
    "for article in articles:\n",
    "    for attribut in erwartete_attribute:\n",
    "        if attribut not in article or not article[attribut]:\n",
    "            fehlende_eintrage[attribut] += 1\n",
    "\n",
    "# Gib die Anzahl der fehlenden Einträge für jedes Attribut aus\n",
    "for attribut, anzahl in fehlende_eintrage.items():\n",
    "    if anzahl > 0:\n",
    "        print(f\"{anzahl} Einträge fehlen im Attribut '{attribut}'.\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Prüfe, ob überhaupt fehlende Einträge existieren\n",
    "if all(anzahl == 0 for anzahl in fehlende_eintrage.values()):\n",
    "    print(\"Alle Artikel haben alle erwarteten Attribute mit Einträgen.\")\n",
    "else:\n",
    "    print(\"Einige Artikel haben fehlende oder leere Attribute.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:37:35.883817Z",
     "start_time": "2024-03-11T19:37:35.854595Z"
    }
   },
   "id": "3396df3a1b060470",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First Import in Neo4j"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d324fc8bd997fbc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:20:05.153018Z",
     "start_time": "2024-03-11T19:20:04.946826Z"
    }
   },
   "id": "524c6fcfa9f8fe9f",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\" \n",
    "user = \"graph1\" \n",
    "password = \"pubmedgraph\" \n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:20:49.203371Z",
     "start_time": "2024-03-11T19:20:49.199269Z"
    }
   },
   "id": "21071fb34f2bbee6",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnung: Artikel 34818923 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34786969 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34785528 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34785529 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34785531 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34777382 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34761633 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34759044 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34759045 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34759046 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34759047 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34757857 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34755562 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34728520 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34728522 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34728523 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34728524 hat kein Jahr. Überspringe 'year'.\n",
      "Warnung: Artikel 34726974 hat kein Jahr. Überspringe 'year'.\n"
     ]
    }
   ],
   "source": [
    "def import_data(driver, articles):\n",
    "    with driver.session() as session:\n",
    "        for article in articles:\n",
    "            # Prüfe, ob 'year' null ist und ersetze es ggf. durch einen Standardwert oder lasse es weg\n",
    "            if article['year'] is None:\n",
    "                print(f\"Warnung: Artikel {article['pmid']} hat kein Jahr. Überspringe 'year'.\")\n",
    "                year_to_insert = \"Unknown\"  # Oder wähle einen anderen passenden Standardwert\n",
    "            else:\n",
    "                year_to_insert = article['year']\n",
    "            \n",
    "            # Erstelle einen Knoten für den Artikel, wobei 'year' ggf. durch einen Standardwert ersetzt wird\n",
    "            session.run(\"MERGE (a:Article {pmid: $pmid, title: $title, abstractText: $abstractText, year: $year, journal: $journal})\", \n",
    "                        pmid=article['pmid'], title=article['title'], abstractText=article['abstractText'], year=year_to_insert, journal=article['journal'])\n",
    "            \n",
    "            # Erstelle Knoten für MeSH-Begriffe und Kanten zu Artikeln\n",
    "            for mesh in article.get('meshMajor', []):  # Verwende get, um einen leeren Standardwert zu liefern, falls 'meshMajor' nicht existiert\n",
    "                session.run(\"MERGE (m:MeSH {term: $term}) MERGE (a:Article {pmid: $pmid}) MERGE (a)-[:TAGGED_WITH]->(m)\", term=mesh, pmid=article['pmid'])\n",
    "\n",
    "\n",
    "# Lese die JSON-Daten ein\n",
    "with open('data/10k_sample_article.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Importiere die Daten in Neo4j\n",
    "import_data(driver, articles)\n",
    "\n",
    "driver.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T19:39:13.609203Z",
     "start_time": "2024-03-11T19:38:23.768536Z"
    }
   },
   "id": "22aec4d1bd3370f7",
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
